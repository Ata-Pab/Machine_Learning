{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ogfidtmKOuE"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zq2LYNOZKOuF",
        "outputId": "76b386b5-ccd2-4fbb-d2c0-20ccaad2a79a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output of Linear Layer for tensor x: \n",
            "\n",
            " tf.Tensor(\n",
            "[[-0.11631936 -0.03059545  0.06272326  0.06283144]\n",
            " [-0.11631936 -0.03059545  0.06272326  0.06283144]], shape=(2, 4), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "A layer encapsulates both a state (the layer's \"weights\") and a transformation from inputs to outputs (a \"call\", the layer's forward pass)\n",
        "Create Linear Layer\n",
        "'''\n",
        "class Linear(keras.layers.Layer):\n",
        "    def __init__(self, units=32, input_dim=32):\n",
        "        super(Linear, self).__init__()\n",
        "        # Weights are initialized\n",
        "        w_init = tf.random_normal_initializer()\n",
        "        self.w = tf.Variable(initial_value = w_init(shape=(input_dim, units), dtype=\"float32\"), trainable=True,)\n",
        "        # bias value are initialized\n",
        "        b_init = tf.zeros_initializer()\n",
        "        self.b = tf.Variable(initial_value = b_init(shape=(units,), dtype=\"float32\"), trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.matmul(inputs, self.w) + self.b   # Matrix multiplication: a.x1 + b\n",
        "\n",
        "# Usage of Linear Layer\n",
        "x = tf.ones((2,2))\n",
        "linear_layer = Linear(4,2)\n",
        "output = linear_layer(x)  # 'call' method is automatically invoked\n",
        "print(\"Output of Linear Layer for tensor x: \\n\\n\", output)\n",
        "\n",
        "#if condition returns True, then nothing happens, if False, AssertionError is raised\n",
        "assert linear_layer.weights == [linear_layer.w, linear_layer.b]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-ooTEFeKOuG"
      },
      "source": [
        "Adding weight to a layer using add_weight() function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgJ7sSJIKOuH",
        "outputId": "d65e4803-3a21-4837-80ae-6b9c52e95db1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[-0.00187546  0.06312266  0.01194087  0.04764556]\n",
            " [-0.00187546  0.06312266  0.01194087  0.04764556]], shape=(2, 4), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "class Linear(keras.layers.Layer):\n",
        "    def __init__(self, units=32, input_dim=32):\n",
        "        super(Linear, self).__init__()\n",
        "        self.w = self.add_weight(shape=(input_dim, units), initializer=\"random_normal\", trainable=True)   # Random Normal weights at initial layer\n",
        "        self.b = self.add_weight(shape=(units,), initializer=\"zeros\", trainable=True)   # Initial bias value is 0\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.matmul(inputs, self.w) + self.b\n",
        "\n",
        "\n",
        "x = tf.ones((2, 2))\n",
        "linear_layer = Linear(4, 2)\n",
        "y = linear_layer(x)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqidIgE0KOuH"
      },
      "source": [
        "Layers can have non-trainable weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMQFCxKFKOuH",
        "outputId": "2b7ae834-4b2e-409a-c4ea-a0c8e8154cb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2. 2.]\n",
            "[4. 4.]\n",
            "[6. 6.]\n"
          ]
        }
      ],
      "source": [
        "# Trainable parameter is set to \n",
        "class ComputeSum(keras.layers.Layer):\n",
        "    def __init__(self, input_dim):\n",
        "        super(ComputeSum, self).__init__()\n",
        "        self.total = tf.Variable(initial_value=tf.zeros((input_dim,)), trainable=False)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n",
        "        return self.total\n",
        "\n",
        "'''\n",
        "tf.assign_add() is a TensorFlow operation that adds a value to a variable and returns the updated value. \n",
        "The function has the following signature:\n",
        "tf.assign_add(\n",
        "    ref,\n",
        "    value,\n",
        "    use_locking=False,\n",
        "    name=None\n",
        ")\n",
        "\n",
        "where:\n",
        "* ref is a tf.Variable object, which represents the variable to be updated.\n",
        "* value is a Tensor, which represents the value to be added to the variable.\n",
        "* use_locking (optional) is a boolean that controls whether the operation should use locking.\n",
        "* name (optional) is a string that defines the name of the operation.\n",
        "'''\n",
        "\n",
        "x = tf.ones((2, 2))\n",
        "my_sum = ComputeSum(2)   # Input dimension is 2\n",
        "\n",
        "y = my_sum(x)     # Add (2,2) to (0,0) => (2,2)\n",
        "print(y.numpy())\n",
        "\n",
        "y = my_sum(x)     # Add (2,2) to (2,2) => (4,4)\n",
        "print(y.numpy())\n",
        "\n",
        "y = my_sum(x)     # Add (2,2) to (4,4) => (6,6)\n",
        "print(y.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeWNofZDKOuI",
        "outputId": "4f4d2a8b-0db5-42eb-bd37-4341bab81700",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weights: 1\n",
            "non-trainable weights: 1\n",
            "trainable_weights: []\n"
          ]
        }
      ],
      "source": [
        "print(\"weights:\", len(my_sum.weights))\n",
        "print(\"non-trainable weights:\", len(my_sum.non_trainable_weights))\n",
        "\n",
        "# It's not included in the trainable weights:\n",
        "print(\"trainable_weights:\", my_sum.trainable_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDQP89cEKOuI"
      },
      "source": [
        "In many cases, you may not know in advance the size of your inputs, and you would like to lazily create weights when that value becomes known, some time after instantiating the layer.\n",
        "\n",
        "In the Keras API, we recommend creating layer weights in the build(self, inputs_shape) method of your layer. Like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyuMynsEKOuI"
      },
      "outputs": [],
      "source": [
        "class Linear(keras.layers.Layer):\n",
        "    def __init__(self, units=32):\n",
        "        super(Linear, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "    # The __call__() method of your layer will automatically run build func in the first time it is called\n",
        "    def build(self, input_shape):\n",
        "        self.w = self.add_weight(\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.b = self.add_weight(shape=(self.units,), initializer=\"random_normal\", trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.matmul(inputs, self.w) + self.b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dbSnfs9KOuI"
      },
      "outputs": [],
      "source": [
        "# At instantiation, we don't know on what inputs this is going to get called\n",
        "linear_layer = Linear(32)\n",
        "\n",
        "# The layer's weights are created dynamically the first time the layer is called\n",
        "y = linear_layer(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MyRIlGmKOuI"
      },
      "source": [
        "Layers are recursively composable.\n",
        "We recommend creating such sublayers in the __init__() method and leave it to the first __call__() to trigger building their weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Teg9bfaVKOuJ",
        "outputId": "38c79d08-1470-4587-fb1e-54c2e9699ded",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weights: 6\n",
            "trainable weights: 6\n"
          ]
        }
      ],
      "source": [
        "class MLPBlock(keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(MLPBlock, self).__init__()\n",
        "        # The first two layers have 32 neurons each\n",
        "        # and the third layer has 1 neuron.\n",
        "        self.linear_1 = Linear(32)   # You may want to use Dense layer instead of Linear\n",
        "        self.linear_2 = Linear(32)\n",
        "        self.linear_3 = Linear(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.linear_1(inputs)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.linear_2(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        return self.linear_3(x)\n",
        "\n",
        "\n",
        "mlp = MLPBlock()\n",
        "# Tensor of shape (3, 64) is passed as input to the mlp object to create the weights\n",
        "y = mlp(tf.ones(shape=(3, 64)))  # The first call to the `mlp` will create the weights\n",
        "print(\"weights:\", len(mlp.weights))\n",
        "print(\"trainable weights:\", len(mlp.trainable_weights))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO7jP9L2KOuJ"
      },
      "source": [
        "add_loss() method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oexin4qKOuJ",
        "outputId": "59784bce-9f35-44de-b41f-3d037f1d86dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThis property is reset at the start of every __call__() to the top-level layer, \\nso that layer.losses always contains the loss values created during the last forward pass.\\n\\nThis implementation is adding the regularization loss during the forward pass of the network, \\nwhich means that the regularization term will be included in the gradients calculated during \\nbackpropagation and therefore included in the optimization process.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# A layer that creates an activity regularization loss\n",
        "class ActivityRegularizationLayer(keras.layers.Layer):\n",
        "    def __init__(self, rate=1e-2):\n",
        "        super(ActivityRegularizationLayer, self).__init__()\n",
        "        self.rate = rate\n",
        "\n",
        "    def call(self, inputs):\n",
        "        self.add_loss(self.rate * tf.reduce_sum(inputs))\n",
        "        return inputs\n",
        "'''\n",
        "This property is reset at the start of every __call__() to the top-level layer, \n",
        "so that layer.losses always contains the loss values created during the last forward pass.\n",
        "\n",
        "This implementation is adding the regularization loss during the forward pass of the network, \n",
        "which means that the regularization term will be included in the gradients calculated during \n",
        "backpropagation and therefore included in the optimization process.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0QMj9hcKOuJ"
      },
      "outputs": [],
      "source": [
        "class OuterLayer(keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(OuterLayer, self).__init__()\n",
        "        self.activity_reg = ActivityRegularizationLayer(1e-2)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.activity_reg(inputs)\n",
        "\n",
        "layer = OuterLayer()\n",
        "assert len(layer.losses) == 0  # No losses yet since the layer has never been called\n",
        "\n",
        "_ = layer(tf.zeros(1, 1))\n",
        "assert len(layer.losses) == 1  # We created one loss value\n",
        "\n",
        "# `layer.losses` gets reset at the start of each __call__\n",
        "_ = layer(tf.zeros(1, 1))\n",
        "assert len(layer.losses) == 1  # This is the loss created during the call above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISj4UFBAKOuJ"
      },
      "source": [
        "In addition, the loss property also contains regularization losses created for the weights of any inner layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MORelcApKOuK",
        "outputId": "a748e907-eac5-4af5-c99c-0db5aec35ae7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<tf.Tensor: shape=(), dtype=float32, numpy=0.0024622313>]\n"
          ]
        }
      ],
      "source": [
        "class OuterLayerWithKernelRegularizer(keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(OuterLayerWithKernelRegularizer, self).__init__()\n",
        "        # Dense Layer is like Linear layer (If we set activation to None in the dense layer in keras API, then they are technically equivalent)\n",
        "        self.dense = keras.layers.Dense(32, kernel_regularizer=tf.keras.regularizers.l2(1e-3))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.dense(inputs)\n",
        "\n",
        "layer = OuterLayerWithKernelRegularizer()\n",
        "_ = layer(tf.zeros((1, 1)))\n",
        "\n",
        "# This is `1e-3 * sum(layer.dense.kernel ** 2)`,\n",
        "# created by the `kernel_regularizer` above.\n",
        "print(layer.losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQKZt16mKOuK"
      },
      "outputs": [],
      "source": [
        "# ! Do not run this cell, it just show an example of GradientTape usage\n",
        "\n",
        "# Instantiate an optimizer.\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Iterate over the batches of a dataset.\n",
        "for x_batch_train, y_batch_train in train_dataset:\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = layer(x_batch_train)  # Logits for this minibatch\n",
        "    # Loss value for this minibatch\n",
        "    loss_value = loss_fn(y_batch_train, logits)\n",
        "    # Add extra losses created during this forward pass:\n",
        "    loss_value += sum(model.losses)\n",
        "\n",
        "  grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "\n",
        "# For More detailed training loops:\n",
        "# https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoFjba4iKOuK",
        "outputId": "cab61fe0-cbaf-4d87-a400-422f889d899a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 167ms/step - loss: 0.0623\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0153\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f72f8e6ab80>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "inputs = keras.Input(shape=(3,))\n",
        "outputs = ActivityRegularizationLayer()(inputs)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "# If there is a loss passed in `compile`, the regularization\n",
        "# losses get added to it\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "model.fit(np.random.random((2, 3)), np.random.random((2, 3)))\n",
        "\n",
        "# It's also possible not to pass any loss in `compile`,\n",
        "# since the model already has a loss to minimize, via the `add_loss`\n",
        "# call during the forward pass!\n",
        "model.compile(optimizer=\"adam\")\n",
        "model.fit(np.random.random((2, 3)), np.random.random((2, 3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-slNEgfjKOuK"
      },
      "source": [
        "add_metric() method\n",
        "\n",
        "Layers also have an add_metric() method for tracking the moving average of a quantity during training.\n",
        "\n",
        "Consider the following layer: a \"logistic endpoint\" layer. It takes as inputs predictions & targets, it computes a loss which it tracks via add_loss(), and it computes an accuracy scalar, which it tracks via add_metric()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dc3iAdDFKOuK"
      },
      "outputs": [],
      "source": [
        "class LogisticEndpoint(keras.layers.Layer):\n",
        "    def __init__(self, name=None):\n",
        "        super(LogisticEndpoint, self).__init__(name=name)\n",
        "        self.loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "        self.accuracy_fn = keras.metrics.BinaryAccuracy()\n",
        "\n",
        "    def call(self, targets, logits, sample_weights=None):\n",
        "        # Compute the training-time loss value and add it\n",
        "        # to the layer using `self.add_loss()`.\n",
        "        loss = self.loss_fn(targets, logits, sample_weights)\n",
        "        self.add_loss(loss)\n",
        "\n",
        "        # Log accuracy as a metric and add it\n",
        "        # to the layer using `self.add_metric()`.\n",
        "        acc = self.accuracy_fn(targets, logits, sample_weights)\n",
        "        self.add_metric(acc, name=\"accuracy\")\n",
        "\n",
        "        # Return the inference-time prediction tensor (for `.predict()`).\n",
        "        return tf.nn.softmax(logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47Hbl8jwKOuL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "280af395-ddeb-4ee1-f1ac-9677e0e89e7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layer.metrics: [<keras.metrics.accuracy_metrics.BinaryAccuracy object at 0x7f72e94c5fd0>]\n",
            "Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "layer = LogisticEndpoint()\n",
        "\n",
        "targets = tf.ones((2,2))\n",
        "logits = tf.ones((2,2))\n",
        "\n",
        "y = layer(targets, logits)\n",
        "\n",
        "print(\"layer.metrics:\", layer.metrics)\n",
        "print(\"Accuracy:\", float(layer.metrics[0].result()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QB5HxOC6KOuL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5026c5cb-e031-477f-d2dc-79458e7cf215"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 632ms/step - loss: 0.9029 - binary_accuracy: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f72e7ff43a0>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "inputs = keras.Input(shape=(3,), name='inputs')\n",
        "targets = keras.Input(shape=(10,), name='targets')\n",
        "logits = keras.layers.Dense(10)(inputs)\n",
        "predictions = LogisticEndpoint(name='predictions')(logits, targets)\n",
        "\n",
        "model = keras.Model(inputs=[inputs, targets], outputs=predictions)\n",
        "model.compile(optimizer='adam')\n",
        "\n",
        "data = {\n",
        "    'inputs': np.random.random((3,3)),\n",
        "    'targets': np.random.random((3,10)),\n",
        "}\n",
        "\n",
        "model.fit(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can optionally enable serialization on your layers\n",
        "\n",
        "get_config() method"
      ],
      "metadata": {
        "id": "LZp16qQ7icfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear(keras.layers.Layer):\n",
        "  def __init__(self, units=32):\n",
        "    super(Linear, self).__init__()\n",
        "    self.units = units\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
        "                             initializer='random_normal',\n",
        "                             trainable=True)\n",
        "    self.b = self.add_weight(shape=(self.units,), initializer='random_normal', trainable=True)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return tf.matmul(inputs, self.w) + self.b\n",
        "\n",
        "  def get_config(self):\n",
        "    return {'units': self.units}"
      ],
      "metadata": {
        "id": "kEX18IwtiaOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer = Linear(64)\n",
        "\n",
        "config = layer.get_config()\n",
        "\n",
        "print(config)\n",
        "new_layer = Linear.from_config(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pqq0aG4wiaMA",
        "outputId": "0b94e0b7-1236-4f1f-93ba-08fe95698b1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'units': 64}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear(keras.layers.Layer):\n",
        "    def __init__(self, units=32, **kwargs):\n",
        "        super(Linear, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.w = self.add_weight(\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.b = self.add_weight(\n",
        "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.matmul(inputs, self.w) + self.b\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(Linear, self).get_config()\n",
        "        config.update({\"units\": self.units})\n",
        "        return config\n"
      ],
      "metadata": {
        "id": "Obs5v6iSiaJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer = Linear(64)\n",
        "\n",
        "config = layer.get_config()\n",
        "\n",
        "print(config)\n",
        "new_layer = Linear.from_config(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxY2qD3EiaGj",
        "outputId": "674c146c-6ab3-4a21-db2a-25e551b48c43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'linear_8', 'trainable': True, 'dtype': 'float32', 'units': 64}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Privileged training argument in the call() method\n",
        "\n",
        "**BatchNormalization** layer and the **Dropout** layer, have different behaviors during **training** and **inference**. \n",
        "\n",
        "Expose a **training** (boolean) argument in the call() method."
      ],
      "metadata": {
        "id": "vTOHSDf7mCQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDropout(keras.layers.Layer):\n",
        "  def __init__(self, rate, **kwargs):\n",
        "    super(CustomDropout, self).__init__(**kwargs)\n",
        "    self.rate = rate\n",
        "\n",
        "  def call(self, inputs, training=None):\n",
        "    if training:\n",
        "      return tf.nn.dropout(inputs, rate=self.rate)\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "FQKCgWEYiaEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The other privileged argument supported by call() is the mask argument.\n",
        "\n",
        "You will find it in all Keras RNN layers. A mask is a boolean tensor (one boolean value per timestep in the input) used to skip certain input timesteps when processing timeseries data.\n",
        "\n",
        "Keras will automatically pass the correct mask argument to __call__() for layers that support it, when a mask is generated by a prior layer. Mask-generating layers are the Embedding layer configured with mask_zero=True, and the Masking layer.\n",
        "\n",
        "To learn more about masking and how to write masking-enabled layers, please check out the guide \"understanding padding and masking\".\n",
        "\n",
        "https://www.tensorflow.org/guide/keras/masking_and_padding?hl=en\n",
        "\n",
        "### Model class\n",
        "\n",
        "The Model class has the same API as Layer, with the following differences:\n",
        "\n",
        "* It exposes built-in training, evaluation, and prediction loops (model.fit(), model.evaluate(), model.predict()).\n",
        "* It exposes the list of its inner layers, via the model.layers property.\n",
        "* It exposes saving and serialization APIs (save(), save_weights()...)"
      ],
      "metadata": {
        "id": "NrbKZ5ovnRqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ! Do not run this cell, it just show an example of GradientTape usage\n",
        "\n",
        "class ResNet(tf.keras.Model):\n",
        "  def __init__(self, num_classes=1000):\n",
        "    super(ResNet, self).__init__()\n",
        "    self.block_1 = ResNetBlock()\n",
        "    self.block_2 = ResNetBlock()\n",
        "    self.global_pool = layers.GloabalAveragePooling2D()\n",
        "    self.classifier = keras.layers.Dense(num_classes)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.block_1(inputs)\n",
        "    x = self.block_2(x)\n",
        "    x = self.global_pool(x)\n",
        "    return self.classifier(x)\n",
        "\n",
        "\n",
        "resnet = ResNet()\n",
        "dataset = ...\n",
        "resnet.fit(dataset, epochs=10)\n",
        "resnet.save(filepath)"
      ],
      "metadata": {
        "id": "kWCZaSzzmbrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## End-to-end example\n",
        "\n",
        "* Layers can be recursively nested to create new, bigger computation blocks.\n",
        "* Layers can create and track losses (typically regularization losses) as well as metrics, via add_loss() and add_metric()\n",
        "* The outer container, the thing you want to train, is a Model. A Model is just like a Layer, but with added training and serialization utilities.\n",
        "\n",
        "### Variational Autoencoder\n",
        "\n",
        "Dataset: MNIST\n",
        "\n",
        "VAE will be a subclass of model and it will feature a regularization loss (KL divergence)\n",
        "\n",
        "![](https://pub.mdpi-res.com/make/make-02-00020/article_deploy/html/images/make-02-00020-g001-550.jpg?1602276661)"
      ],
      "metadata": {
        "id": "EmhSKncpo2LW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import backend as K\n",
        "\n",
        "# z_mean, z_log_var to sample z, the vector encoding a digit\n",
        "class Sampling(keras.layers.Layer):\n",
        "\n",
        "  def call(self, inputs):\n",
        "    z_mean, z_log_var = inputs\n",
        "    batch = tf.shape(z_mean)[0]\n",
        "    dim = tf.shape(z_mean)[1]\n",
        "    epsilon = K.random_normal(shape=(batch, dim))\n",
        "\n",
        "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
      ],
      "metadata": {
        "id": "fDip4nuUmbpg"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder Layer"
      ],
      "metadata": {
        "id": "mcwA-fGtE068"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Maps MNIST Digits to a triplet (z_mean, z_log_var, z).\n",
        "# Latent dimension: 32\n",
        "# Intermediate dimension: 64\n",
        "class Encoder(keras.layers.Layer):\n",
        "  def __init__(self, latent_dim=32, intermediate_dim=64, name='encoder', **kwargs):\n",
        "    super(Encoder, self).__init__(name=name, **kwargs)\n",
        "    self.dense_proj = keras.layers.Dense(intermediate_dim, activation='relu')\n",
        "    self.dense_mean = keras.layers.Dense(latent_dim)\n",
        "    self.dense_log_var = keras.layers.Dense(latent_dim)\n",
        "    self.sampling = Sampling()  # create Samling object\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.dense_proj(inputs)\n",
        "    z_mean = self.dense_mean(x)\n",
        "    z_log_var = self.dense_log_var(x)\n",
        "    z = self.sampling((z_mean, z_log_var))\n",
        "    return z_mean, z_log_var, z"
      ],
      "metadata": {
        "id": "kKDQu0GumbnI"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder Layer"
      ],
      "metadata": {
        "id": "sDW4gza2E4K5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converts z, the encoded digit vector, back into readable digit\n",
        "class Decoder(keras.layers.Layer):\n",
        "  def __init__(self, original_dim, intermediate_dim=64, name='decoder', **kwargs):\n",
        "    super(Decoder, self).__init__(name=name, **kwargs)\n",
        "    self.dense_proj = keras.layers.Dense(intermediate_dim, activation='relu')\n",
        "    self.dense_output = keras.layers.Dense(original_dim, activation='sigmoid')\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.dense_proj(inputs)\n",
        "    return self.dense_output(x)"
      ],
      "metadata": {
        "id": "D98llJ_pmbke"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Â VariationalAutoEncoder Model"
      ],
      "metadata": {
        "id": "CWYS1pYBE70A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combines the encoder and decoder into a model for training\n",
        "class VariationalAutoEncoder(keras.Model):\n",
        "  def __init__(self, original_dim, intermediate_dim=64, latent_dim=32, name='autoencoder', **kwargs):\n",
        "    super(VariationalAutoEncoder, self).__init__(name=name, **kwargs)\n",
        "    self.original_dim = original_dim\n",
        "    self.encoder = Encoder(latent_dim=latent_dim, intermediate_dim=intermediate_dim)\n",
        "    self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    z_mean, z_log_var, z = self.encoder(inputs)  # Encode input data\n",
        "    reconstructed = self.decoder(z)     # Decode (reconstruct) the data\n",
        "    # Add KL Divergence regularization loss\n",
        "    kl_loss = -0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
        "    self.add_loss(kl_loss)  # Add KL Divergence loss to the model\n",
        "    return reconstructed"
      ],
      "metadata": {
        "id": "7wbahJU3mbh-"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure global parameters and functions"
      ],
      "metadata": {
        "id": "Vh7w1BkRFAYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_dim = 784   # 28x28 pixels\n",
        "learning_rate = 1e-3 # 0.001\n",
        "epochs = 2\n",
        "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "mse_loss_fn = keras.losses.MeanSquaredError()   # MSE\n",
        "loss_metric = keras.metrics.Mean()"
      ],
      "metadata": {
        "id": "dZFdZ8D-BfG6"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare data"
      ],
      "metadata": {
        "id": "Oxpntb2fHLWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, _), _ = keras.datasets.mnist.load_data()\n",
        "x_train =  x_train.reshape(60000, 784).astype('float32') / 255.0\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)"
      ],
      "metadata": {
        "id": "QoQwWsbWBfE6"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lF9ceFidF2IW",
        "outputId": "cfb8f5bf-523f-4f78-f4a4-fe50f3bf86cd"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_BatchDataset element_spec=TensorSpec(shape=(None, 784), dtype=tf.float32, name=None)>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Variational AutoEncoder model"
      ],
      "metadata": {
        "id": "ip6tvKNjGygV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vae = VariationalAutoEncoder(original_dim, intermediate_dim=64, latent_dim=32)"
      ],
      "metadata": {
        "id": "LnsxurEPBfCJ"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "  print('Epoch %d' % (epoch,))\n",
        "\n",
        "  # Iterate over the batches of the dataset\n",
        "  for step, x_batch_train in enumerate(train_dataset):\n",
        "    with tf.GradientTape() as tape:\n",
        "      reconstructed = vae(x_batch_train)\n",
        "      # Compute reconstruction loss\n",
        "      loss = mse_loss_fn(x_batch_train, reconstructed)\n",
        "      loss += sum(vae.losses)  # Add KL Divergence regularization loss\n",
        "    \n",
        "    # Backpropagation (update weights using loss)\n",
        "    grads = tape.gradient(loss, vae.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
        "\n",
        "    loss_metric(loss)\n",
        "\n",
        "    if step % 100 == 0:\n",
        "      print(\"step %d: mean loss = %.4f\" % (step, loss_metric.result()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aWSJ4q6CXMI",
        "outputId": "c7c35f9a-5449-4ac5-cf56-7a67bee26254"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n",
            "step 0: mean loss = 0.3819\n",
            "step 100: mean loss = 0.1278\n",
            "step 200: mean loss = 0.1003\n",
            "step 300: mean loss = 0.0899\n",
            "step 400: mean loss = 0.0848\n",
            "step 500: mean loss = 0.0814\n",
            "step 600: mean loss = 0.0791\n",
            "step 700: mean loss = 0.0775\n",
            "step 800: mean loss = 0.0763\n",
            "step 900: mean loss = 0.0752\n",
            "Epoch 1\n",
            "step 0: mean loss = 0.0749\n",
            "step 100: mean loss = 0.0742\n",
            "step 200: mean loss = 0.0737\n",
            "step 300: mean loss = 0.0732\n",
            "step 400: mean loss = 0.0729\n",
            "step 500: mean loss = 0.0725\n",
            "step 600: mean loss = 0.0722\n",
            "step 700: mean loss = 0.0719\n",
            "step 800: mean loss = 0.0716\n",
            "step 900: mean loss = 0.0713\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Alternative way to train VariationalAutoEncoder model"
      ],
      "metadata": {
        "id": "AfZwNXWrHRPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vae = VariationalAutoEncoder(original_dim=784, intermediate_dim=64, latent_dim=32)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\n",
        "\n",
        "vae.fit(x_train, x_train, epochs=2, batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nh2ECmFACXHG",
        "outputId": "88234bc8-48fe-4074-d4d8-9deeef8cf986"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "938/938 [==============================] - 9s 8ms/step - loss: 0.0746\n",
            "Epoch 2/2\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0676\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f32993ef4c0>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remake the example without object oriented structure"
      ],
      "metadata": {
        "id": "Bh1weITtH4B8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_dim = 784\n",
        "intermediate_dim = 64\n",
        "latent_dim = 32"
      ],
      "metadata": {
        "id": "3_tGJmmfID_I"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder model"
      ],
      "metadata": {
        "id": "Dizj9PA9IHlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_inputs = tf.keras.Input(shape=(original_dim, ), name='encoder_input')\n",
        "x = keras.layers.Dense(intermediate_dim, activation='relu')(original_inputs)\n",
        "z_mean = keras.layers.Dense(latent_dim, name='z_mean')(x)\n",
        "z_log_var = keras.layers.Dense(latent_dim, name='z_log_var')(x)\n",
        "z = Sampling()((z_mean, z_log_var))\n",
        "encoder = keras.Model(inputs=original_inputs, outputs=z, name='encoder')"
      ],
      "metadata": {
        "id": "zX7i4iiGID4n"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder Model"
      ],
      "metadata": {
        "id": "hwuumFdjIJSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latent_inputs = keras.Input(shape=(latent_dim, ), name='z_sampling')\n",
        "x = keras.layers.Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
        "outputs = keras.layers.Dense(original_dim, activation='sigmoid')(x)\n",
        "decoder = keras.Model(inputs=latent_inputs, outputs=outputs, name='decoder')"
      ],
      "metadata": {
        "id": "TO8Qh1SyIPgJ"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variational Autoencoder Model"
      ],
      "metadata": {
        "id": "Sp0-KmsNJzTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = decoder(z)\n",
        "vae = keras.Model(inputs=original_inputs, outputs=outputs, name='vae')"
      ],
      "metadata": {
        "id": "QKdv0rqiJ0Ob"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KL Divergence Regularization Loss"
      ],
      "metadata": {
        "id": "M3_syaX6IQ8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kl_loss = -0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
        "vae.add_loss(kl_loss)"
      ],
      "metadata": {
        "id": "vFFtkgAJIPdo"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the Model"
      ],
      "metadata": {
        "id": "YcSdA6lKIURb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
        "vae.compile(optimizer, loss=keras.losses.MeanSquaredError())\n",
        "vae.fit(x_train, x_train, epochs=3, batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcM44WIZIPbJ",
        "outputId": "4c1f6e68-8974-4cae-b310-1f5ef039bbf7"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "938/938 [==============================] - 7s 6ms/step - loss: 0.0748\n",
            "Epoch 2/3\n",
            "938/938 [==============================] - 9s 9ms/step - loss: 0.0676\n",
            "Epoch 3/3\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0675\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f329955d610>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reference\n",
        "\n",
        "https://www.tensorflow.org/guide/keras/custom_layers_and_models?hl=en"
      ],
      "metadata": {
        "id": "3O9mIIbYKdBK"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tensorflow",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "d5b9f1ef5fa62f4137fca86c7cff097ad613f28b09a73c8ab6cd8441b2451117"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}