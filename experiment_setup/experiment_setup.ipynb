{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import os\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense"
      ],
      "metadata": {
        "id": "TS39tIvIas1R"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-i9vVoAYaty1",
        "outputId": "0adfbde1-b585-4f60-9fcc-763eec09a847"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.makedirs('utils/', exist_ok=True)\n",
        "os.chdir('utils')\n",
        "\n",
        "! wget -q https://raw.githubusercontent.com/Ata-Pab/Machine_Learning/master/utils/models.py\n",
        "! wget -q https://raw.githubusercontent.com/Ata-Pab/Machine_Learning/master/utils/losses.py\n",
        "! wget -q https://raw.githubusercontent.com/Ata-Pab/Machine_Learning/master/utils/vision.py\n",
        "! wget -q https://raw.githubusercontent.com/Ata-Pab/Machine_Learning/master/utils/callbacks.py\n",
        "! wget -q https://raw.githubusercontent.com/Ata-Pab/Machine_Learning/master/utils/utils.py\n",
        "\n",
        "os.chdir('/content')\n",
        "print(\"Current working directory\", os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sS0FzQQ0aunn",
        "outputId": "b5d58763-1daa-431e-984d-4b10ae59567e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from utils import vision\n",
        "from utils import utils\n",
        "from utils import losses"
      ],
      "metadata": {
        "id": "cKOGmQ4savqy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment Setup Start"
      ],
      "metadata": {
        "id": "UdzcQjBPiwDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create experiment folder\n",
        "EXPERIMENT_NAME = '...'\n",
        "EXPERIMENT_SAVE_DIR = os.path.join('/content/drive/MyDrive/MASTER/Master_Thesis/Experiments', EXPERIMENT_NAME)"
      ],
      "metadata": {
        "id": "rzr8gHuVfaWW"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Id5kWdfHSY0V"
      },
      "outputs": [],
      "source": [
        "experiment = {\n",
        "    'TYPE': 'test',        # Experiment type: 'train', 'test'\n",
        "    'ACCELERATOR': 'GPU',   # 'CPU', 'GPU' or 'TPU'\n",
        "\n",
        "    # Input data\n",
        "    'DATASET': 'BD67_Dataset',\n",
        "    'IMAGE_SIZE': (256, 256),\n",
        "    'INPUT_SHAPE': (256, 256, 3),\n",
        "    'VALID_SIZE': 0.1,      # Validation data size: (Valid Data) / (All Data)\n",
        "    'DATA_AUG': True,       # Apply data augmentation\n",
        "    'DATA_AUG_POWER': 1,    # Data augmentation power: How many times data\n",
        "     # augmentation will be applied to the whole dataset. default 1\n",
        "\n",
        "    # Model\n",
        "    'BACKBONE': 'custom',        # 'custom', 'VGG16', 'VGG19' - default 'custom'\n",
        "    'LAST_TRANIABLE_LAYERS': 5,  # number of last trainable layers of pre-trained backbone models, fine-tuning\n",
        "    'BATCH_SIZE': 16,            # IF TPU is active set 4, otherwise set anything\n",
        "    'EPOCHS': 2,\n",
        "    'OPTIMIZER': 'Adam',  # TODO: Try 'rmsprop' optimizer\n",
        "    'LEARNING_RATE': 1e-4,\n",
        "    # set latent dim - shape: (LATENT_DIM, 1) - default 200\n",
        "    'LATENT_DIM': 500,\n",
        "\n",
        "    # Loss\n",
        "    'RECONS_LOSS': 'PERCEPTUAL',  # Reconstruction loss ('SSIM', 'MSE', 'MAE', 'PERCEPTUAL')\n",
        "    'PERCEPTUAL_LAYERS': [5,8,13,18],    # [5,8,13,18], None\n",
        "    'PERCEP_LOSS_MODEL': 'VGG19', # 'custom', 'VGG16', 'VGG19', 'ResNet50' - default 'VGG16'\n",
        "    'PERP_LOSS_LAMBDA': 1,\n",
        "    'LRELU_SLOPE': 0.2,       # Leaky ReLU activation function slope value\n",
        "    'MSE_LOSS_LAMBDA': 0.01,  # MSE coeff\n",
        "\n",
        "    # Save model\n",
        "    'SAVE_WEIGHTS_PER_EPOCH': 1,  # Checkpoints\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if experiment['TYPE'] == 'train':\n",
        "    assert(EXPERIMENT_NAME != '...')\n",
        "    # Create experiment folder\n",
        "    os.makedirs(EXPERIMENT_SAVE_DIR, exist_ok=True)\n",
        "\n",
        "    # Model checkpoints will be save in exp_save_dir\n",
        "    exp_save_dir = utils.create_experimental_output(experiment, EXPERIMENT_SAVE_DIR)\n",
        "\n",
        "    TRAINING_WEIGHT_DIR = os.path.join(exp_save_dir, 'training_weights')\n",
        "    # Create folder for checkpoints (training weights)\n",
        "    os.makedirs(TRAINING_WEIGHT_DIR, exist_ok=True)\n",
        "else:  # test mode\n",
        "    # Set experiment save directory and training weight directory manually\n",
        "    exp_save_dir = '/content/drive/MyDrive/MASTER/Master_Thesis/Experiments/SiamDecoderResNet50Model/experiment_1'\n",
        "    TRAINING_WEIGHT_DIR = os.path.join(exp_save_dir, 'training_weights')"
      ],
      "metadata": {
        "id": "ogIlwduWipj7"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"...Experiment {exp_save_dir.split('experiment_')[1]} was initialized...\")\n",
        "print(f\"Experiment directory: {EXPERIMENT_SAVE_DIR}\")\n",
        "print(f\"Training weights save directory: {TRAINING_WEIGHT_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMKtxPWHp_de",
        "outputId": "050861ed-7666-4cba-b26c-eaf327b7beb2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "...Experiment 1 was initialized...\n",
            "Experiment directory: /content/drive/MyDrive/MASTER/Master_Thesis/Experiments/...\n",
            "Training weights save directory: /content/drive/MyDrive/MASTER/Master_Thesis/Experiments/SiamDecoderResNet50Model/experiment_1/training_weights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment Setup End"
      ],
      "metadata": {
        "id": "HVvToQxci2dS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Custom model"
      ],
      "metadata": {
        "id": "2YIjiRIe5h-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomModel(Model):\n",
        "    \"\"\"The Siamese Network model with a custom training and testing loops.\n",
        "\n",
        "    Computes the triplet loss using the three embeddings produced by the\n",
        "    Siamese Network.\n",
        "\n",
        "    The triplet loss is defined as:\n",
        "       L(A, P, N) = max(‖f(A) - f(P)‖² - ‖f(A) - f(N)‖² + margin, 0)\n",
        "    \"\"\"\n",
        "    def __init__(self, input_shape):\n",
        "        ...\n",
        "        return 0"
      ],
      "metadata": {
        "id": "OjgWhPVLjkSh"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def debug_training_process(model, epoch_num, data_input, metrics):\n",
        "  ...\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "YnHtV3E86hMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if experiment['ACCELERATOR'] != 'TPU':\n",
        "  @tf.function\n",
        "  def train_step(images):\n",
        "      ...\n",
        "\n",
        "      return 0"
      ],
      "metadata": {
        "id": "ZqxbsuS3j2Nc"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython import display\n",
        "import time\n",
        "\n",
        "if experiment['ACCELERATOR'] != 'TPU':\n",
        "  def train(dataset, epochs):\n",
        "      loss_hist = []  # Keep loss history\n",
        "      for epoch in range(epochs):\n",
        "          start = time.time()\n",
        "          for image_batch in dataset:\n",
        "              loss = train_step(image_batch)\n",
        "\n",
        "          loss_hist.append(loss)   # Add loss value to the loss history after each epoch\n",
        "          print(\"loss: \", loss)\n",
        "\n",
        "          # Save the model every experiment['SAVE_WEIGHTS_PER_EPOCH'] epochs\n",
        "          if (epoch + 1) % experiment['SAVE_WEIGHTS_PER_EPOCH'] == 0:\n",
        "            seed = image_batch[:experiment['BATCH_SIZE']]\n",
        "            display.clear_output(wait=True)\n",
        "            debug_training_process(siamese_resnet50_model,\n",
        "                                      epoch + 1,\n",
        "                                      seed,\n",
        "                                      loss)\n",
        "\n",
        "            # Save checkpoints\n",
        "            utils.save_experiment_checkpoints([...], epoch=(epoch+1), save_dir=TRAINING_WEIGHT_DIR)\n",
        "            print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "      # Generate after the final epoch\n",
        "      display.clear_output(wait=True)\n",
        "      debug_training_process(siamese_resnet50_model,\n",
        "                              epochs,\n",
        "                              seed,\n",
        "                              loss)\n",
        "      return loss_hist"
      ],
      "metadata": {
        "id": "YxYSCe6lj2LI"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if experiment['TYPE'] == 'train':\n",
        "    custom_model.compile(ae_optimizer=tf.keras.optimizers.Adam(0.0001), siam_optimizer=tf.keras.optimizers.Adam(0.0001))\n",
        "    #custom_model.fit(..., epochs=experiment['EPOCHS'], validation_data=...)\n",
        "    custom_model_hist = train(..., experiment['EPOCHS'])\n",
        "else:  # test mode\n",
        "    # Set load weight epoch number manually\n",
        "    utils.load_model_experiment_weights([...], epoch=..., load_dir=TRAINING_WEIGHT_DIR)"
      ],
      "metadata": {
        "id": "EhM4jgeIj4wC"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}